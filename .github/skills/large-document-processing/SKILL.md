# Skill: Large Document Processing

大規模ドキュメント（数百〜数千行の Markdown など）を解析・翻訳・要約・解説する際に、コンテキストウィンドウの飽和を回避しながら高品質なアウトプットを生成するための戦略。

## When to apply

以下のいずれかに該当する場合、このスキルを適用する:

- 入力ソースが 200 行を超える Markdown / テキストファイル
- 出力が複数セクションにまたがる長文ドキュメント
- 翻訳・詳細解説・AI 分析サイドカーなど、入力全体を網羅する必要があるタスク

## Strategy: 4-phase chunked processing

### Phase 1 — 構造解析 (Structural Analysis)

ソースファイルを**全文読み込まずに**構造を把握する。

1. `wc -l` でファイル全体の行数を確認する
2. `grep -n '^## \|^### '` で見出しと行番号を抽出し、セクション境界をマッピングする
3. セクション数・各セクションの行数範囲を把握し、処理チャンクの計画を立てる

**目的**: コンテキストを消費せずに全体像を掴み、以降のチャンク分割を最適化する。

### Phase 2 — 分割読み込み (Chunked Reading)

Phase 1 で特定したセクション境界に沿って、`read_file` の `offset` / `limit` パラメータを使い 100〜200 行単位で読み込む。

- セクションの意味的なまとまりを壊さないようにチャンク境界を設定する
- 各チャンクを読み込んだら、その内容を**理解・解釈してから**次のチャンクに進む
- 一度に全文を読み込まない

**目的**: コンテキストウィンドウ内に収まる単位で確実に内容を理解する。

### Phase 3 — 逐次出力 (Incremental Output)

各チャンクの理解が完了したら、**即座に**出力ファイルへ書き出す。

1. 最初のチャンク → `create_file` でファイルを作成し、冒頭部分（front matter、概要、最初のセクション）を出力
2. 以降のチャンク → `replace_string_in_file` で末尾に追記
3. チャンクごとに書き出すことで、中間成果をファイルに永続化する

**目的**: 処理済みの内容をコンテキストから解放し、後続チャンクの処理余地を確保する。

### Phase 4 — マッシュアップ (Mashup & Finalization)

全チャンクの処理完了後、文書全体を統合・仕上げする。

- 横断的な集約セクションの追加（設定項目一覧テーブル、Breaking Changes、まとめなど）
- ドキュメント構造の一貫性チェック
- 統計情報の報告（行数、セクション数、設定項目数など）

**目的**: 個別チャンクの出力を一つの完成したドキュメントに昇華する。

## Guidelines

### チャンク処理中の原則

- **1 チャンク = 1 読み込み + 1 理解 + 1 書き出し** のサイクルを守る
- 前のチャンクの詳細な内容を記憶に頼らない。出力ファイルに書き出し済みであることを信頼する
- `manage_todo_list` でセクション単位の進捗を追跡する

### コンテキスト効率のためのテクニック

- `grep` や `wc` などの軽量コマンドで事前にメタ情報を収集し、`read_file` の呼び出し回数を最小化する
- 互いに独立した情報収集（行数確認、見出し抽出など）は並列で実行する
- 出力ファイルへの書き出しが完了したチャンクの詳細は再読み込みしない

### 品質担保

- Phase 1 の構造解析で把握したセクション一覧を、最終的な出力のセクション一覧と突合し、漏れがないことを確認する
- Phase 4 で `wc -l` と `grep` で最終出力の統計を取得し、報告する
