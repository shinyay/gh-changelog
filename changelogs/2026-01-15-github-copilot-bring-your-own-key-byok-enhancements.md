---
title: "GitHub Copilot bring your own key (BYOK) enhancements"
date: "2026-01-15"
type: "new releases"
labels: ["copilot"]
author: "Allison"
source_url: "https://github.blog/changelog/2026-01-15-github-copilot-bring-your-own-key-byok-enhancements"
fetched_at: "2026-02-03T14:40:05.183887Z"
---

# GitHub Copilot bring your own key (BYOK) enhancements

## Overview
Bring your own key (BYOK) for GitHub Copilot now supports additional APIs, new configuration options, and more provider integrations.

## Detailed Explanation
### Overview
- Bring your own key (BYOK) for GitHub Copilot now supports additional APIs, new configuration options, and more provider integrations.
- With this update, enterprises can connect models that use the Responses API, configure a maximum context window for model interactions, enable streaming, and choose from an expanded list of supported LLM providers.

### What’s new
- New provider options You can now connect API keys from AWS Bedrock, Google AI Studio, and any OpenAI‑compatible provider. These options join Anthropic, Microsoft Foundry, OpenAI, and xAI as supported BYOK choices.
- Support for the Responses API BYOK now supports models that use the Responses API. This enables structured outputs and richer multimodal interactions.
- Ability to set a maximum context window Admins can define the maximum context window for BYOK models, helping balance cost, performance, and response quality.
- Streaming responses for faster interaction Streaming output from connected models lets Copilot display responses as they are generated, rather than waiting for completion.

### Start using the new BYOK features today
- These new capabilities are available now in public preview for GitHub Enterprise and Business customers. Connect your LLM provider’s API key in your enterprise or organization settings and start using your models in Copilot Chat and supported IDEs. Learn more by visiting our BYOK documentation .

### Help us shape the future
- We are just getting started and your feedback will guide what comes next. Join the discussion within the GitHub Community to share feedback and connect with other developers.

## Key Changes
- These new capabilities are available now in public preview for GitHub Enterprise and Business customers. Connect your LLM provider’s API key in your enterprise or organization settings and start using your models in Copilot Chat and supported IDEs. Learn more by visiting our BYOK documentation .

## Impact / Who’s Affected
- Start using the new BYOK features today These new capabilities are available now in public preview for GitHub Enterprise and Business customers.

## Insights (derived from article language)
- Learn more by visiting our BYOK documentation .

## Article Content (cleaned)
Bring your own key (BYOK) for GitHub Copilot now supports additional APIs, new configuration options, and more provider integrations.


With this update, enterprises can connect models that use the Responses API, configure a maximum context window for model interactions, enable streaming, and choose from an expanded list of supported LLM providers.


### [What’s new](#whats-new)


**New provider options**  

You can now connect API keys from AWS Bedrock, Google AI Studio, and any OpenAI‑compatible provider. These options join Anthropic, Microsoft Foundry, OpenAI, and xAI as supported BYOK choices.


**Support for the Responses API**  

BYOK now supports models that use the Responses API. This enables structured outputs and richer multimodal interactions.


**Ability to set a maximum context window**  

Admins can define the maximum context window for BYOK models, helping balance cost, performance, and response quality.


**Streaming responses for faster interaction**  

Streaming output from connected models lets Copilot display responses as they are generated, rather than waiting for completion.


### [Start using the new BYOK features today](#start-using-the-new-byok-features-today)


These new capabilities are available now in public preview for GitHub Enterprise and Business customers. Connect your LLM provider’s API key in your enterprise or organization settings and start using your models in Copilot Chat and supported IDEs. Learn more by visiting [our BYOK documentation](https://docs.github.com/copilot/how-tos/administer-copilot/manage-for-enterprise/use-your-own-api-keys).


### [Help us shape the future](#help-us-shape-the-future)


We are just getting started and your feedback will guide what comes next. Join the discussion within the [GitHub Community](https://github.com/orgs/community/discussions/184350) to share feedback and connect with other developers.
